{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from utils import *\n",
    "from models import *\n",
    "from attention import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== THE DATASET IS READY ========\n"
     ]
    }
   ],
   "source": [
    "# Download the file\n",
    "get_file()\n",
    "path_to_file = 'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of line in the dataset is 118964\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 118960\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107064 107064 11896 11896\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "2 ----> <start>\n",
      "7 ----> el\n",
      "15 ----> es\n",
      "10172 ----> futbolista\n",
      "4 ----> .\n",
      "3 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "2 ----> <start>\n",
      "13 ----> he\n",
      "12 ----> is\n",
      "10 ----> a\n",
      "860 ----> soccer\n",
      "1137 ----> player\n",
      "4 ----> .\n",
      "3 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)// BATCH_SIZE\n",
    "embedding_dim_source = 256\n",
    "embedding_dim_target = 100\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "CPU times: user 13.6 s, sys: 267 ms, total: 13.9 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "glove_dir = '../glove_weights/'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt'), 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "for line in lines:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_tar_size, embedding_dim))\n",
    "for word, i in targ_lang.word_index.items():\n",
    "    if i < vocab_tar_size:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to torch tensor\n",
    "tensor_x = torch.Tensor(input_tensor_train).long() \n",
    "tensor_y = torch.Tensor(target_tensor_train).long()\n",
    "# create your datset\n",
    "my_dataset = data.TensorDataset(tensor_x,tensor_y) \n",
    "# create your dataloader\n",
    "my_dataloader = data.DataLoader(my_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        drop_last=True,\n",
    "                        num_workers=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(my_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 42])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) torch.Size([64, 42, 1024])\n",
      "Encoder Hidden state shape: (batch size, units) torch.Size([1, 64, 1024])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim_source, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) torch.Size([64, 1024])\n",
      "Attention weights shape: (batch_size, sequence_length, 1) torch.Size([64, 42, 1])\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10, 1024)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) torch.Size([64, 13853])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim_target, units, BATCH_SIZE, 1024)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(torch.randint(1, 20, (BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim_source, units, BATCH_SIZE).to(device)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim_target, units, BATCH_SIZE, 1024).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the pretrained glove's weights in the Decoder\n",
    "decoder.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, device=device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 9.3025\n",
      "Epoch 1 Batch 100 Loss 1.7025\n",
      "Epoch 1 Batch 200 Loss 1.5339\n",
      "Epoch 1 Batch 300 Loss 1.2020\n",
      "Epoch 1 Batch 400 Loss 1.0292\n",
      "Epoch 1 Batch 500 Loss 1.0688\n",
      "Epoch 1 Batch 600 Loss 1.2681\n",
      "Epoch 1 Batch 700 Loss 0.9368\n",
      "Epoch 1 Batch 800 Loss 1.2883\n",
      "Epoch 1 Batch 900 Loss 1.0381\n",
      "Epoch 1 Batch 1000 Loss 0.9625\n",
      "Epoch 1 Batch 1100 Loss 0.9252\n",
      "Epoch 1 Batch 1200 Loss 0.9168\n",
      "Epoch 1 Batch 1300 Loss 0.8842\n",
      "Epoch 1 Batch 1400 Loss 1.1590\n",
      "Epoch 1 Batch 1500 Loss 1.3492\n",
      "Epoch 1 Batch 1600 Loss 0.7430\n",
      "Epoch 1 Loss 1.2110\n",
      "Time taken for 1 epoch 1076.4737224578857 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.7252\n",
      "Epoch 2 Batch 100 Loss 1.5175\n",
      "Epoch 2 Batch 200 Loss 0.8572\n",
      "Epoch 2 Batch 300 Loss 0.7244\n",
      "Epoch 2 Batch 400 Loss 1.2360\n",
      "Epoch 2 Batch 500 Loss 0.7041\n",
      "Epoch 2 Batch 600 Loss 0.9075\n",
      "Epoch 2 Batch 700 Loss 0.7779\n",
      "Epoch 2 Batch 800 Loss 0.7562\n",
      "Epoch 2 Batch 900 Loss 0.8102\n",
      "Epoch 2 Batch 1000 Loss 0.5955\n",
      "Epoch 2 Batch 1100 Loss 0.6611\n",
      "Epoch 2 Batch 1200 Loss 0.9849\n",
      "Epoch 2 Batch 1300 Loss 0.7949\n",
      "Epoch 2 Batch 1400 Loss 0.7716\n",
      "Epoch 2 Batch 1500 Loss 0.6162\n",
      "Epoch 2 Batch 1600 Loss 0.6182\n",
      "Epoch 2 Loss 0.8370\n",
      "Time taken for 1 epoch 1090.1333878040314 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.8043\n",
      "Epoch 3 Batch 100 Loss 0.7055\n",
      "Epoch 3 Batch 200 Loss 0.6595\n",
      "Epoch 3 Batch 300 Loss 0.6682\n",
      "Epoch 3 Batch 400 Loss 0.6524\n",
      "Epoch 3 Batch 500 Loss 0.5936\n",
      "Epoch 3 Batch 600 Loss 0.5288\n",
      "Epoch 3 Batch 700 Loss 0.6813\n",
      "Epoch 3 Batch 800 Loss 0.6453\n",
      "Epoch 3 Batch 900 Loss 0.4911\n",
      "Epoch 3 Batch 1000 Loss 0.5072\n",
      "Epoch 3 Batch 1100 Loss 0.7035\n",
      "Epoch 3 Batch 1200 Loss 0.5514\n",
      "Epoch 3 Batch 1300 Loss 0.4472\n",
      "Epoch 3 Batch 1400 Loss 0.4299\n",
      "Epoch 3 Batch 1500 Loss 0.5950\n",
      "Epoch 3 Batch 1600 Loss 0.7768\n",
      "Epoch 3 Loss 0.6219\n",
      "Time taken for 1 epoch 1086.0057759284973 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.4493\n",
      "Epoch 4 Batch 100 Loss 0.4582\n",
      "Epoch 4 Batch 200 Loss 0.3546\n",
      "Epoch 4 Batch 300 Loss 0.5387\n",
      "Epoch 4 Batch 400 Loss 0.5047\n",
      "Epoch 4 Batch 500 Loss 0.5078\n",
      "Epoch 4 Batch 600 Loss 0.6018\n",
      "Epoch 4 Batch 700 Loss 0.5502\n",
      "Epoch 4 Batch 800 Loss 0.3714\n",
      "Epoch 4 Batch 900 Loss 0.5701\n",
      "Epoch 4 Batch 1000 Loss 0.5461\n",
      "Epoch 4 Batch 1100 Loss 0.6401\n",
      "Epoch 4 Batch 1200 Loss 0.4588\n",
      "Epoch 4 Batch 1300 Loss 0.3706\n",
      "Epoch 4 Batch 1400 Loss 0.5618\n",
      "Epoch 4 Batch 1500 Loss 0.6262\n",
      "Epoch 4 Batch 1600 Loss 0.5047\n",
      "Epoch 4 Loss 0.5171\n",
      "Time taken for 1 epoch 1087.3430774211884 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.3714\n",
      "Epoch 5 Batch 100 Loss 0.5176\n",
      "Epoch 5 Batch 200 Loss 0.3232\n",
      "Epoch 5 Batch 300 Loss 0.3809\n",
      "Epoch 5 Batch 400 Loss 0.4006\n",
      "Epoch 5 Batch 500 Loss 0.4005\n",
      "Epoch 5 Batch 600 Loss 0.4007\n",
      "Epoch 5 Batch 700 Loss 0.5816\n",
      "Epoch 5 Batch 800 Loss 0.2851\n",
      "Epoch 5 Batch 900 Loss 0.3049\n",
      "Epoch 5 Batch 1000 Loss 0.5572\n",
      "Epoch 5 Batch 1100 Loss 0.3889\n",
      "Epoch 5 Batch 1200 Loss 0.4901\n",
      "Epoch 5 Batch 1300 Loss 0.5425\n",
      "Epoch 5 Batch 1400 Loss 0.3084\n",
      "Epoch 5 Batch 1500 Loss 0.4506\n",
      "Epoch 5 Batch 1600 Loss 0.3749\n",
      "Epoch 5 Loss 0.4251\n",
      "Time taken for 1 epoch 1066.9183375835419 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.3709\n",
      "Epoch 6 Batch 100 Loss 0.4856\n",
      "Epoch 6 Batch 200 Loss 0.3129\n",
      "Epoch 6 Batch 300 Loss 0.4146\n",
      "Epoch 6 Batch 400 Loss 0.2629\n",
      "Epoch 6 Batch 500 Loss 0.4295\n",
      "Epoch 6 Batch 600 Loss 0.2563\n",
      "Epoch 6 Batch 700 Loss 0.2936\n",
      "Epoch 6 Batch 800 Loss 0.3871\n",
      "Epoch 6 Batch 900 Loss 0.3089\n",
      "Epoch 6 Batch 1000 Loss 0.2687\n",
      "Epoch 6 Batch 1100 Loss 0.3104\n",
      "Epoch 6 Batch 1200 Loss 0.4208\n",
      "Epoch 6 Batch 1300 Loss 0.4658\n",
      "Epoch 6 Batch 1400 Loss 0.4173\n",
      "Epoch 6 Batch 1500 Loss 0.2993\n",
      "Epoch 6 Batch 1600 Loss 0.4675\n",
      "Epoch 6 Loss 0.3695\n",
      "Time taken for 1 epoch 1066.844141960144 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.2422\n",
      "Epoch 7 Batch 100 Loss 0.4356\n",
      "Epoch 7 Batch 200 Loss 0.3411\n",
      "Epoch 7 Batch 300 Loss 0.3165\n",
      "Epoch 7 Batch 400 Loss 0.2050\n",
      "Epoch 7 Batch 500 Loss 0.4149\n",
      "Epoch 7 Batch 600 Loss 0.2404\n",
      "Epoch 7 Batch 700 Loss 0.2821\n",
      "Epoch 7 Batch 800 Loss 0.2275\n",
      "Epoch 7 Batch 900 Loss 0.2491\n",
      "Epoch 7 Batch 1000 Loss 0.4193\n",
      "Epoch 7 Batch 1100 Loss 0.3112\n",
      "Epoch 7 Batch 1200 Loss 0.2410\n",
      "Epoch 7 Batch 1300 Loss 0.3106\n",
      "Epoch 7 Batch 1400 Loss 0.2517\n",
      "Epoch 7 Batch 1500 Loss 0.4054\n",
      "Epoch 7 Batch 1600 Loss 0.2425\n",
      "Epoch 7 Loss 0.3095\n",
      "Time taken for 1 epoch 1066.3449738025665 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.3843\n",
      "Epoch 8 Batch 100 Loss 0.3445\n",
      "Epoch 8 Batch 200 Loss 0.3540\n",
      "Epoch 8 Batch 300 Loss 0.2637\n",
      "Epoch 8 Batch 400 Loss 0.1904\n",
      "Epoch 8 Batch 500 Loss 0.1956\n",
      "Epoch 8 Batch 600 Loss 0.3124\n",
      "Epoch 8 Batch 700 Loss 0.1692\n",
      "Epoch 8 Batch 800 Loss 0.2108\n",
      "Epoch 8 Batch 900 Loss 0.1853\n",
      "Epoch 8 Batch 1000 Loss 0.3137\n",
      "Epoch 8 Batch 1100 Loss 0.1975\n",
      "Epoch 8 Batch 1200 Loss 0.2176\n",
      "Epoch 8 Batch 1300 Loss 0.2610\n",
      "Epoch 8 Batch 1400 Loss 0.2658\n",
      "Epoch 8 Batch 1500 Loss 0.4050\n",
      "Epoch 8 Batch 1600 Loss 0.2130\n",
      "Epoch 8 Loss 0.2789\n",
      "Time taken for 1 epoch 1065.9756894111633 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.3920\n",
      "Epoch 9 Batch 100 Loss 0.2284\n",
      "Epoch 9 Batch 200 Loss 0.3661\n",
      "Epoch 9 Batch 300 Loss 0.2305\n",
      "Epoch 9 Batch 400 Loss 0.1528\n",
      "Epoch 9 Batch 500 Loss 0.3072\n",
      "Epoch 9 Batch 600 Loss 0.3066\n",
      "Epoch 9 Batch 700 Loss 0.1442\n",
      "Epoch 9 Batch 800 Loss 0.1645\n",
      "Epoch 9 Batch 900 Loss 0.3589\n",
      "Epoch 9 Batch 1000 Loss 0.3147\n",
      "Epoch 9 Batch 1100 Loss 0.2190\n",
      "Epoch 9 Batch 1200 Loss 0.3210\n",
      "Epoch 9 Batch 1300 Loss 0.1899\n",
      "Epoch 9 Batch 1400 Loss 0.3326\n",
      "Epoch 9 Batch 1500 Loss 0.3115\n",
      "Epoch 9 Batch 1600 Loss 0.3393\n",
      "Epoch 9 Loss 0.2504\n",
      "Time taken for 1 epoch 1065.6827428340912 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1477\n",
      "Epoch 10 Batch 100 Loss 0.1826\n",
      "Epoch 10 Batch 200 Loss 0.2350\n",
      "Epoch 10 Batch 300 Loss 0.1888\n",
      "Epoch 10 Batch 400 Loss 0.1405\n",
      "Epoch 10 Batch 500 Loss 0.1908\n",
      "Epoch 10 Batch 600 Loss 0.1525\n",
      "Epoch 10 Batch 700 Loss 0.1133\n",
      "Epoch 10 Batch 800 Loss 0.1468\n",
      "Epoch 10 Batch 900 Loss 0.2158\n",
      "Epoch 10 Batch 1000 Loss 0.1587\n",
      "Epoch 10 Batch 1100 Loss 0.2615\n",
      "Epoch 10 Batch 1200 Loss 0.2288\n",
      "Epoch 10 Batch 1300 Loss 0.1779\n",
      "Epoch 10 Batch 1400 Loss 0.1571\n",
      "Epoch 10 Batch 1500 Loss 0.1772\n",
      "Epoch 10 Batch 1600 Loss 0.1749\n",
      "Epoch 10 Loss 0.2224\n",
      "Time taken for 1 epoch 1064.5764424800873 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.1396\n",
      "Epoch 11 Batch 100 Loss 0.2472\n",
      "Epoch 11 Batch 200 Loss 0.3039\n",
      "Epoch 11 Batch 300 Loss 0.1189\n",
      "Epoch 11 Batch 400 Loss 0.1619\n",
      "Epoch 11 Batch 500 Loss 0.1117\n",
      "Epoch 11 Batch 600 Loss 0.1217\n",
      "Epoch 11 Batch 700 Loss 0.3020\n",
      "Epoch 11 Batch 800 Loss 0.2715\n",
      "Epoch 11 Batch 900 Loss 0.2620\n",
      "Epoch 11 Batch 1000 Loss 0.1368\n",
      "Epoch 11 Batch 1100 Loss 0.2154\n",
      "Epoch 11 Batch 1200 Loss 0.1481\n",
      "Epoch 11 Batch 1300 Loss 0.1348\n",
      "Epoch 11 Batch 1400 Loss 0.2604\n",
      "Epoch 11 Batch 1500 Loss 0.3520\n",
      "Epoch 11 Batch 1600 Loss 0.0997\n",
      "Epoch 11 Loss 0.2012\n",
      "Time taken for 1 epoch 1064.5842452049255 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.1492\n",
      "Epoch 12 Batch 100 Loss 0.2299\n",
      "Epoch 12 Batch 200 Loss 0.1304\n",
      "Epoch 12 Batch 300 Loss 0.1135\n",
      "Epoch 12 Batch 400 Loss 0.1050\n",
      "Epoch 12 Batch 500 Loss 0.2762\n",
      "Epoch 12 Batch 600 Loss 0.1164\n",
      "Epoch 12 Batch 700 Loss 0.1270\n",
      "Epoch 12 Batch 800 Loss 0.1849\n",
      "Epoch 12 Batch 900 Loss 0.1396\n",
      "Epoch 12 Batch 1000 Loss 0.0869\n",
      "Epoch 12 Batch 1100 Loss 0.1469\n",
      "Epoch 12 Batch 1200 Loss 0.1136\n",
      "Epoch 12 Batch 1300 Loss 0.1675\n",
      "Epoch 12 Batch 1400 Loss 0.2173\n",
      "Epoch 12 Batch 1500 Loss 0.1145\n",
      "Epoch 12 Batch 1600 Loss 0.2880\n",
      "Epoch 12 Loss 0.1879\n",
      "Time taken for 1 epoch 1064.4185271263123 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.1405\n",
      "Epoch 13 Batch 100 Loss 0.1923\n",
      "Epoch 13 Batch 200 Loss 0.2050\n",
      "Epoch 13 Batch 300 Loss 0.2324\n",
      "Epoch 13 Batch 400 Loss 0.1128\n",
      "Epoch 13 Batch 500 Loss 0.1358\n",
      "Epoch 13 Batch 600 Loss 0.2235\n",
      "Epoch 13 Batch 700 Loss 0.1077\n",
      "Epoch 13 Batch 800 Loss 0.2111\n",
      "Epoch 13 Batch 900 Loss 0.2702\n",
      "Epoch 13 Batch 1000 Loss 0.1934\n",
      "Epoch 13 Batch 1100 Loss 0.2805\n",
      "Epoch 13 Batch 1200 Loss 0.1624\n",
      "Epoch 13 Batch 1300 Loss 0.1331\n",
      "Epoch 13 Batch 1400 Loss 0.1500\n",
      "Epoch 13 Batch 1500 Loss 0.1382\n",
      "Epoch 13 Batch 1600 Loss 0.1014\n",
      "Epoch 13 Loss 0.1763\n",
      "Time taken for 1 epoch 1064.2058749198914 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.1091\n",
      "Epoch 14 Batch 100 Loss 0.1387\n",
      "Epoch 14 Batch 200 Loss 0.1327\n",
      "Epoch 14 Batch 300 Loss 0.0882\n",
      "Epoch 14 Batch 400 Loss 0.2286\n",
      "Epoch 14 Batch 500 Loss 0.1766\n",
      "Epoch 14 Batch 600 Loss 0.1667\n",
      "Epoch 14 Batch 700 Loss 0.1505\n",
      "Epoch 14 Batch 800 Loss 0.2145\n",
      "Epoch 14 Batch 900 Loss 0.1132\n",
      "Epoch 14 Batch 1000 Loss 0.1435\n",
      "Epoch 14 Batch 1100 Loss 0.2093\n",
      "Epoch 14 Batch 1200 Loss 0.0965\n",
      "Epoch 14 Batch 1300 Loss 0.1267\n",
      "Epoch 14 Batch 1400 Loss 0.1052\n",
      "Epoch 14 Batch 1500 Loss 0.0873\n",
      "Epoch 14 Batch 1600 Loss 0.1219\n",
      "Epoch 14 Loss 0.1604\n",
      "Time taken for 1 epoch 1063.658572435379 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 0 Loss 0.0905\n",
      "Epoch 15 Batch 100 Loss 0.0830\n",
      "Epoch 15 Batch 200 Loss 0.1780\n",
      "Epoch 15 Batch 300 Loss 0.0790\n",
      "Epoch 15 Batch 400 Loss 0.1168\n",
      "Epoch 15 Batch 500 Loss 0.1208\n",
      "Epoch 15 Batch 600 Loss 0.1693\n",
      "Epoch 15 Batch 700 Loss 0.1393\n",
      "Epoch 15 Batch 800 Loss 0.0982\n",
      "Epoch 15 Batch 900 Loss 0.2855\n",
      "Epoch 15 Batch 1000 Loss 0.2173\n",
      "Epoch 15 Batch 1100 Loss 0.1001\n",
      "Epoch 15 Batch 1200 Loss 0.1540\n",
      "Epoch 15 Batch 1300 Loss 0.1156\n",
      "Epoch 15 Batch 1400 Loss 0.1073\n",
      "Epoch 15 Batch 1500 Loss 0.2110\n",
      "Epoch 15 Batch 1600 Loss 0.1207\n",
      "Epoch 15 Loss 0.1537\n",
      "Time taken for 1 epoch 1063.4489080905914 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.1609\n",
      "Epoch 16 Batch 100 Loss 0.0792\n",
      "Epoch 16 Batch 200 Loss 0.0823\n",
      "Epoch 16 Batch 300 Loss 0.2293\n",
      "Epoch 16 Batch 400 Loss 0.1668\n",
      "Epoch 16 Batch 500 Loss 0.0761\n",
      "Epoch 16 Batch 600 Loss 0.1602\n",
      "Epoch 16 Batch 700 Loss 0.1616\n",
      "Epoch 16 Batch 800 Loss 0.1561\n",
      "Epoch 16 Batch 900 Loss 0.1376\n",
      "Epoch 16 Batch 1000 Loss 0.1659\n",
      "Epoch 16 Batch 1100 Loss 0.1049\n",
      "Epoch 16 Batch 1200 Loss 0.0557\n",
      "Epoch 16 Batch 1300 Loss 0.1467\n",
      "Epoch 16 Batch 1400 Loss 0.1994\n",
      "Epoch 16 Batch 1500 Loss 0.1055\n",
      "Epoch 16 Batch 1600 Loss 0.0810\n",
      "Epoch 16 Loss 0.1411\n",
      "Time taken for 1 epoch 1063.2416939735413 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.2277\n",
      "Epoch 17 Batch 100 Loss 0.0779\n",
      "Epoch 17 Batch 200 Loss 0.0730\n",
      "Epoch 17 Batch 300 Loss 0.1312\n",
      "Epoch 17 Batch 400 Loss 0.1359\n",
      "Epoch 17 Batch 500 Loss 0.1875\n",
      "Epoch 17 Batch 600 Loss 0.0943\n",
      "Epoch 17 Batch 700 Loss 0.1411\n",
      "Epoch 17 Batch 800 Loss 0.1837\n",
      "Epoch 17 Batch 900 Loss 0.0851\n",
      "Epoch 17 Batch 1000 Loss 0.0997\n",
      "Epoch 17 Batch 1100 Loss 0.1014\n",
      "Epoch 17 Batch 1200 Loss 0.1975\n",
      "Epoch 17 Batch 1300 Loss 0.1705\n",
      "Epoch 17 Batch 1400 Loss 0.1048\n",
      "Epoch 17 Batch 1500 Loss 0.0912\n",
      "Epoch 17 Batch 1600 Loss 0.1699\n",
      "Epoch 17 Loss 0.1407\n",
      "Time taken for 1 epoch 1063.45627784729 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.1594\n",
      "Epoch 18 Batch 100 Loss 0.1795\n",
      "Epoch 18 Batch 200 Loss 0.0584\n",
      "Epoch 18 Batch 300 Loss 0.0884\n",
      "Epoch 18 Batch 400 Loss 0.1663\n",
      "Epoch 18 Batch 500 Loss 0.1078\n",
      "Epoch 18 Batch 600 Loss 0.1743\n",
      "Epoch 18 Batch 700 Loss 0.1025\n",
      "Epoch 18 Batch 800 Loss 0.1398\n",
      "Epoch 18 Batch 900 Loss 0.1777\n",
      "Epoch 18 Batch 1000 Loss 0.1853\n",
      "Epoch 18 Batch 1100 Loss 0.2372\n",
      "Epoch 18 Batch 1200 Loss 0.0925\n",
      "Epoch 18 Batch 1300 Loss 0.1175\n",
      "Epoch 18 Batch 1400 Loss 0.1850\n",
      "Epoch 18 Batch 1500 Loss 0.0921\n",
      "Epoch 18 Batch 1600 Loss 0.1043\n",
      "Epoch 18 Loss 0.1280\n",
      "Time taken for 1 epoch 1062.9854350090027 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0815\n",
      "Epoch 19 Batch 100 Loss 0.1470\n",
      "Epoch 19 Batch 200 Loss 0.0850\n",
      "Epoch 19 Batch 300 Loss 0.1571\n",
      "Epoch 19 Batch 400 Loss 0.1796\n",
      "Epoch 19 Batch 500 Loss 0.2877\n",
      "Epoch 19 Batch 600 Loss 0.1727\n",
      "Epoch 19 Batch 700 Loss 0.0883\n",
      "Epoch 19 Batch 800 Loss 0.0923\n",
      "Epoch 19 Batch 900 Loss 0.1841\n",
      "Epoch 19 Batch 1000 Loss 0.2058\n",
      "Epoch 19 Batch 1100 Loss 0.0609\n",
      "Epoch 19 Batch 1200 Loss 0.1776\n",
      "Epoch 19 Batch 1300 Loss 0.1530\n",
      "Epoch 19 Batch 1400 Loss 0.2407\n",
      "Epoch 19 Batch 1500 Loss 0.1179\n",
      "Epoch 19 Batch 1600 Loss 0.1333\n",
      "Epoch 19 Loss 0.1304\n",
      "Time taken for 1 epoch 1062.6713416576385 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0569\n",
      "Epoch 20 Batch 100 Loss 0.0558\n",
      "Epoch 20 Batch 200 Loss 0.0441\n",
      "Epoch 20 Batch 300 Loss 0.0966\n",
      "Epoch 20 Batch 400 Loss 0.0636\n",
      "Epoch 20 Batch 500 Loss 0.0707\n",
      "Epoch 20 Batch 600 Loss 0.1662\n",
      "Epoch 20 Batch 700 Loss 0.0638\n",
      "Epoch 20 Batch 800 Loss 0.0758\n",
      "Epoch 20 Batch 900 Loss 0.1233\n",
      "Epoch 20 Batch 1000 Loss 0.0750\n",
      "Epoch 20 Batch 1100 Loss 0.1019\n",
      "Epoch 20 Batch 1200 Loss 0.2383\n",
      "Epoch 20 Batch 1300 Loss 0.0742\n",
      "Epoch 20 Batch 1400 Loss 0.1684\n",
      "Epoch 20 Batch 1500 Loss 0.0913\n",
      "Epoch 20 Batch 1600 Loss 0.1250\n",
      "Epoch 20 Loss 0.1240\n",
      "Time taken for 1 epoch 1062.6299760341644 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(my_dataloader):\n",
    "        inp, targ = inp.to(device), targ.to(device)\n",
    "        batch_loss = train_step(inp, targ, encoder, decoder,\n",
    "                                encoder_optimizer, decoder_optimizer,\n",
    "                                criterion, device, BATCH_SIZE, targ_lang)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss))\n",
    "            \n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        \n",
    "        pass\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'trata de averiguarlo .', max_length_targ, max_length_inp, encoder,\n",
    "          decoder, inp_lang, targ_lang, device, beam_width=10, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'trata de averiguarlo .', max_length_targ, max_length_inp, encoder,\n",
    "          decoder, inp_lang, targ_lang, device, beam_width=100, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'trata de averiguarlo .', max_length_targ, max_length_inp, encoder,\n",
    "          decoder, inp_lang, targ_lang, device, beam_search=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'esta es mi vida .', max_length_targ, max_length_inp, encoder, decoder, inp_lang, targ_lang, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'esta es mi vida .', max_length_targ, max_length_inp, encoder,\n",
    "          decoder, inp_lang, targ_lang, device, beam_width=10, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'hace mucho frio aqui .', max_length_targ, max_length_inp, encoder, decoder, inp_lang, targ_lang, device, beam_search=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "translate(u'hace mucho frio aqui .', max_length_targ, max_length_inp, encoder, decoder, inp_lang, targ_lang, device, beam_width=5, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
